{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [SpaceNet](https://aws.amazon.com/public-datasets/spacenet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The current SpaceNet corpus includes **thousands of square kilometers of high resolution imagery** collected from **DigitalGlobeâ€™s commercial satellites** which includes **8-band multispectral data**. This dataset is being made public to advance the development of **algorithms to automatically extract geometric features such as roads, building footprints, and points of interest using satellite imagery**. The currently available Areas of Interest (AOI) are **Rio De Janeiro**, Paris, Las Vegas, Shanghai and Khartoum.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Dependencies\n",
    "The [AWS Command Line Interface (CLI)](https://aws.amazon.com/cli/) must be installed with an active AWS account. Configure the AWS CLI using `aws configure`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Accessing the SpaceNet Data on AWS\n",
    "The imagery is [GeoTIFF](https://en.wikipedia.org/wiki/GeoTIFF) satellite imagery and corresponding [GeoJSON](https://en.wikipedia.org/wiki/GeoJSON) building footprints.\n",
    "\n",
    "The spacenet-dataset S3 bucket is provided as a [Requester Pays bucket](https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html), so we use [Boto](https://boto3.readthedocs.io/en/latest/index.html), the Amazon Web Services (AWS) SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download time: 5 minutes\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "client = boto3.client(\"s3\")\n",
    "# https://boto3.readthedocs.io/en/latest/reference/customizations/s3.html#boto3.s3.transfer.S3Transfer\n",
    "transfer = boto3.s3.transfer.S3Transfer(client)\n",
    "\n",
    "bucket = \"spacenet-dataset\"\n",
    "# 20 tiffs listed in \"AOI_1_Rio_manifest.txt\".\n",
    "names = [\n",
    "    \"013022223310.tif\",\n",
    "    \"013022232021.tif\",\n",
    "    \"013022232201.tif\",\n",
    "    \"013022223131.tif\",\n",
    "    \"013022223113.tif\",\n",
    "    \"013022223103.tif\",\n",
    "    \"013022223133.tif\",\n",
    "    \"013022223132.tif\",\n",
    "    \"013022223301.tif\",\n",
    "    \"013022223112.tif\",\n",
    "    \"013022232020.tif\",\n",
    "    \"013022232200.tif\",\n",
    "    \"013022223123.tif\",\n",
    "    \"013022232022.tif\",\n",
    "    \"013022223130.tif\",\n",
    "    \"013022232002.tif\",\n",
    "    \"013022232023.tif\",\n",
    "    \"013022223311.tif\",\n",
    "    \"013022232003.tif\",\n",
    "    \"013022223121.tif\"\n",
    "]\n",
    "\n",
    "key_prefix = \"AOI_1_Rio/srcData/mosaic_3band/\"\n",
    "!mkdir -p /tmp/spacenet-data\n",
    "filename_prefix = \"/tmp/spacenet-data/\"\n",
    "\n",
    "key_filename_tuples = [\n",
    "    (key_prefix + name, filename_prefix + name)\n",
    "    for name in names]\n",
    "\n",
    "# Download takes 4 minutes for 3-band, 16 minutes for 8-band.\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for (key, filename) in key_filename_tuples:\n",
    "    transfer.download_file(\n",
    "        bucket=bucket, key=key, filename=filename,\n",
    "        extra_args={\"RequestPayer\":\"requester\"}\n",
    "    )\n",
    "\n",
    "end = time.time()\n",
    "download_time = end - start\n",
    "minutes = int(download_time)/60\n",
    "print(\"Download time: %d minutes\" % int(minutes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ingest Images with GeoPySpark\n",
    "\n",
    "[GeoPySpark](https://github.com/locationtech-labs/geopyspark) is a Python language binding library of the Scala library, [GeoTrellis](https://github.com/locationtech/geotrellis), which reads, writes, and operates on raster data as fast as possible using Spark.\n",
    "\n",
    "Refer to [Ingesting a Grayscale Image](https://geopyspark.readthedocs.io/en/latest/tutorials/greyscale_ingest_example.html) tutorial for code breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from geopyspark import geopyspark_conf\n",
    "conf = geopyspark_conf(\"local[*]\", \"spacenet-ingest\")\n",
    "geopysc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o29.collectMetadata.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): scala.MatchError: (53971,53970) (of class scala.Tuple2$mcII$sp)\n\tat geotrellis.raster.io.geotiff.reader.TiffTagsReader$.readTag(TiffTagsReader.scala:114)\n\tat geotrellis.raster.io.geotiff.reader.TiffTagsReader$.read(TiffTagsReader.scala:102)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readGeoTiffInfo(GeoTiffReader.scala:288)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readMultiband(GeoTiffReader.scala:174)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readMultiband(GeoTiffReader.scala:152)\n\tat geotrellis.raster.io.geotiff.MultibandGeoTiff$.apply(MultibandGeoTiff.scala:93)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readFully(RasterReader.scala:98)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readFully(RasterReader.scala:96)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$1$$anonfun$apply$2.apply(HadoopGeoTiffRDD.scala:111)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$1$$anonfun$apply$2.apply(HadoopGeoTiffRDD.scala:110)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n\tat geotrellis.spark.TileLayerMetadata$.collectMetadataWithCRS(TileLayerMetadata.scala:147)\n\tat geotrellis.spark.TileLayerMetadata$.fromRdd(TileLayerMetadata.scala:237)\n\tat geotrellis.spark.package$withCollectMetadataMethods.collectMetadata(package.scala:194)\n\tat geopyspark.geotrellis.ProjectedRasterRDD.collectMetadata(RasterRDD.scala:212)\n\tat geopyspark.geotrellis.RasterRDD.collectMetadata(RasterRDD.scala:188)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: scala.MatchError: (53971,53970) (of class scala.Tuple2$mcII$sp)\n\tat geotrellis.raster.io.geotiff.reader.TiffTagsReader$.readTag(TiffTagsReader.scala:114)\n\tat geotrellis.raster.io.geotiff.reader.TiffTagsReader$.read(TiffTagsReader.scala:102)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readGeoTiffInfo(GeoTiffReader.scala:288)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readMultiband(GeoTiffReader.scala:174)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readMultiband(GeoTiffReader.scala:152)\n\tat geotrellis.raster.io.geotiff.MultibandGeoTiff$.apply(MultibandGeoTiff.scala:93)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readFully(RasterReader.scala:98)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readFully(RasterReader.scala:96)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$1$$anonfun$apply$2.apply(HadoopGeoTiffRDD.scala:111)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$1$$anonfun$apply$2.apply(HadoopGeoTiffRDD.scala:110)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8f436d381741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeopysc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSPATIAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"file:///tmp/spacenet-data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Error: https://github.com/locationtech/geotrellis/issues/2268\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# tile the rdd to the layout defined in the metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/.local/lib/python3.4/site-packages/geopyspark/geotrellis/layer.py\u001b[0m in \u001b[0;36mcollect_metadata\u001b[0;34m(self, extent, layout, crs, tile_size)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mjson_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextent\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mjson_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not collect metadata with {} and {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o29.collectMetadata.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): scala.MatchError: (53971,53970) (of class scala.Tuple2$mcII$sp)\n\tat geotrellis.raster.io.geotiff.reader.TiffTagsReader$.readTag(TiffTagsReader.scala:114)\n\tat geotrellis.raster.io.geotiff.reader.TiffTagsReader$.read(TiffTagsReader.scala:102)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readGeoTiffInfo(GeoTiffReader.scala:288)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readMultiband(GeoTiffReader.scala:174)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readMultiband(GeoTiffReader.scala:152)\n\tat geotrellis.raster.io.geotiff.MultibandGeoTiff$.apply(MultibandGeoTiff.scala:93)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readFully(RasterReader.scala:98)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readFully(RasterReader.scala:96)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$1$$anonfun$apply$2.apply(HadoopGeoTiffRDD.scala:111)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$1$$anonfun$apply$2.apply(HadoopGeoTiffRDD.scala:110)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n\tat geotrellis.spark.TileLayerMetadata$.collectMetadataWithCRS(TileLayerMetadata.scala:147)\n\tat geotrellis.spark.TileLayerMetadata$.fromRdd(TileLayerMetadata.scala:237)\n\tat geotrellis.spark.package$withCollectMetadataMethods.collectMetadata(package.scala:194)\n\tat geopyspark.geotrellis.ProjectedRasterRDD.collectMetadata(RasterRDD.scala:212)\n\tat geopyspark.geotrellis.RasterRDD.collectMetadata(RasterRDD.scala:188)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: scala.MatchError: (53971,53970) (of class scala.Tuple2$mcII$sp)\n\tat geotrellis.raster.io.geotiff.reader.TiffTagsReader$.readTag(TiffTagsReader.scala:114)\n\tat geotrellis.raster.io.geotiff.reader.TiffTagsReader$.read(TiffTagsReader.scala:102)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readGeoTiffInfo(GeoTiffReader.scala:288)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readMultiband(GeoTiffReader.scala:174)\n\tat geotrellis.raster.io.geotiff.reader.GeoTiffReader$.readMultiband(GeoTiffReader.scala:152)\n\tat geotrellis.raster.io.geotiff.MultibandGeoTiff$.apply(MultibandGeoTiff.scala:93)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readFully(RasterReader.scala:98)\n\tat geotrellis.spark.io.RasterReader$$anon$2.readFully(RasterReader.scala:96)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$1$$anonfun$apply$2.apply(HadoopGeoTiffRDD.scala:111)\n\tat geotrellis.spark.io.hadoop.HadoopGeoTiffRDD$$anonfun$apply$1$$anonfun$apply$2.apply(HadoopGeoTiffRDD.scala:110)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Ingest takes X minutes.\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from geopyspark.geotrellis.geotiff import get\n",
    "from geopyspark.geotrellis.constants import SPATIAL, ZOOM\n",
    "from geopyspark.geotrellis.catalog import write\n",
    "\n",
    "# Read the GeoTiff locally\n",
    "rdd = get(geopysc, SPATIAL, \"file:///tmp/spacenet-data/\")\n",
    "# Error: https://github.com/locationtech/geotrellis/issues/2268\n",
    "metadata = rdd.collect_metadata()\n",
    "\n",
    "# tile the rdd to the layout defined in the metadata\n",
    "laid_out = rdd.tile_to_layout(metadata)\n",
    "\n",
    "# reproject the tiled rasters using a ZoomedLayoutScheme\n",
    "reprojected = laid_out.reproject(\"EPSG:3857\", scheme=ZOOM)#.cache().repartition(200)\n",
    "\n",
    "# pyramid the TiledRasterRDD to create 12 new TiledRasterRDDs\n",
    "# one for each zoom level\n",
    "pyramided = reprojected.pyramid(start_zoom=12, end_zoom=1)\n",
    "\n",
    "# Save each TiledRasterRDD locally\n",
    "for tiled in pyramided:\n",
    "    write(\"file:///tmp/spacenet-catalog\", \"spacenet-ingest\", tiled)\n",
    "\n",
    "end = time.time()\n",
    "ingest_time = end - start\n",
    "minutes = int(ingest_time)/60\n",
    "print(\"Ingest time: %d minutes\" % minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeoNotebook + GeoPySpark",
   "language": "python",
   "name": "geonotebook3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
